# Accelerating-SGD-Public

In this project, I tested some of the methods for accelerating stochastic gradient descent on the MNIST dataset. These methods are all used in practice for machine learning systems at even the largest scales, and the goal of this assignment was to get some experience working with them so that I could build intuition for how they work. The methods included Nesterov Momentum on Gradient Descent, Momentum and Sequential Sampling with Stochastic Gradient Descent, and ADAM. All optimizations were done on multinomial logistic regression with l2 regularization.

Unfortunately, since this is a school project, I cannot post any code from this project.
